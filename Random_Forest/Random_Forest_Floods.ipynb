{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Created By Konstantinos Fokeas\n",
    "## credits to: @DigitalSreeni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from scipy import ndimage as nd\n",
    " \n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "## STEP 1:   READ TRAINING IMAGES AND EXTRACT FEATURES  - Sentinel 1\n",
    "################################################################\n",
    "image_dataset_s1 = pd.DataFrame()  #Dataframe to capture image features\n",
    "\n",
    "img_path = \"/content/S1Hand/\" ## CHANGE ME!!!\n",
    "for image in os.listdir(img_path):#iterate through each file \n",
    "    df = pd.DataFrame()  #Temporary data frame to capture information for each loop.\n",
    "    #Reset dataframe to blank after each loop.\n",
    "\n",
    "    vv = imread(img_path + image)[0:1,:,:].reshape((128,128)) \n",
    "    vh = imread(img_path + image)[1:2,:,:].reshape((128,128))\n",
    "    vv_vh = (vv/vh)\n",
    " \n",
    "    #Add pixel values to the data frame\n",
    "    vv_values = vv.reshape(-1)\n",
    "    df['VV'] = vv_values   #Pixel value itself as a feature\n",
    "            \n",
    "    vh_values = vh.reshape(-1)\n",
    "    df['VH'] = vh_values   \n",
    "            \n",
    "    vv_vh_values = vv_vh.reshape(-1)\n",
    "    df['VV/VH'] = vv_vh_values   \n",
    "\n",
    "    #MEDIAN with sigma=3\n",
    "    median_img = nd.median_filter(vh, size=3)\n",
    "    median_img1 = median_img.reshape(-1)\n",
    "    df['Median_s3'] = median_img1\n",
    "\n",
    "    #VARIANCE with size=3\n",
    "    variance_img = nd.generic_filter(vh, np.var, size=3)\n",
    "    variance_img1 = variance_img.reshape(-1)\n",
    "    df['Variance_s3'] = variance_img1  \n",
    "\n",
    "    #ROBERTS EDGE\n",
    "    edge_roberts = roberts(vh)\n",
    "    edge_roberts1 = edge_roberts.reshape(-1)\n",
    "    df['Roberts'] = edge_roberts1\n",
    "\n",
    "    ######################################                    \n",
    "    #Update dataframe for images to include details for each image in the loop\n",
    "    image_dataset_s1 = image_dataset_s1.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "## STEP 1:   READ TRAINING IMAGES AND EXTRACT FEATURES - Sentinel 2\n",
    "################################################################\n",
    "image_dataset_s2 = pd.DataFrame()  #Dataframe to capture image features\n",
    "\n",
    "img_path = \"/content/S2Hand/\" # CHANGE ME!!!\n",
    "for image in os.listdir(img_path):\n",
    "    df = pd.DataFrame()  #Temporary data frame to capture information for each loop.\n",
    "    #Reset dataframe to blank after each loop.\n",
    "\n",
    "    blue = imread(img_path + image)[0:1,:,:].reshape((128,128))    \n",
    "    green = imread(img_path + image)[1:2,:,:].reshape((128,128))\n",
    "    red = imread(img_path + image)[2:3,:,:].reshape((128,128))\n",
    "    nir = imread(img_path + image)[3:4,:,:].reshape((128,128))\n",
    "    swir = imread(img_path + image)[4:5,:,:].reshape((128,128))\n",
    "        \n",
    "    # Sentinel-2 MNDWI = (B03 - B08) / (B03 + B08)\n",
    "    ndwi = (red - swir)/ (red + swir)\n",
    "    ndwi_values = ndwi.reshape(-1)\n",
    "    df['NDWI'] = ndwi_values  \n",
    "\n",
    "    # Sentinel 2 NDVI = (NIR - RED) / (NIR + RED), where RED is B4 and NIR is B8\n",
    "    ndvi = (nir - red)/ (red + nir)\n",
    "    ndvi_values = ndvi.reshape(-1)\n",
    "    df['NDVI'] = ndvi_values \n",
    "\n",
    "    #MEDIAN with sigma=3\n",
    "    median_img = nd.median_filter(nir, size=3)\n",
    "    median_img1 = median_img.reshape(-1)\n",
    "    df['Median_s3'] = median_img1\n",
    "\n",
    "   #VARIANCE with size=3\n",
    "    variance_img = nd.generic_filter(nir, np.var, size=3)\n",
    "    variance_img1 = variance_img.reshape(-1)\n",
    "    df['Variance_s3'] = variance_img1  \n",
    "\n",
    "   #ROBERTS EDGE\n",
    "    edge_roberts = roberts(nir)\n",
    "    edge_roberts1 = edge_roberts.reshape(-1)\n",
    "    df['Roberts'] = edge_roberts1\n",
    "\n",
    "  #START ADDING DATA TO THE DATAFRAME  \n",
    "\n",
    "    blue_values = blue.reshape(-1)\n",
    "    df['Blue'] = blue_values  \n",
    "            \n",
    "    green_values = green.reshape(-1)\n",
    "    df['Green'] = green_values   \n",
    "             \n",
    "        \n",
    "    image_dataset_s2 = image_dataset_s2.append(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# CONCATENATE S2 AND S1 DATAFRAMES\n",
    "##########################################################\n",
    "frames = [image_dataset_s1, image_dataset_s2]\n",
    "\n",
    "image_dataset = pd.concat(frames,axis=1,join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# STEP 2: READ LABELED IMAGES (MASKS) AND CREATE ANOTHER DATAFRAME\n",
    "    # WITH LABEL VALUES AND LABEL FILE NAMES\n",
    "##########################################################\n",
    "mask_dataset = pd.DataFrame()  #Create dataframe to capture mask info.\n",
    "# label_path = \"/content/S1Label\" #iterate through each file to perform some action\n",
    "\n",
    "mask_path = \"/content/S1OtsuLabelHand/\" ### CHANGE ME!!!\n",
    "for mask in os.listdir(mask_path):\n",
    "    df2 = pd.DataFrame()  #Temporary dataframe to capture info for each mask in the loop\n",
    "    input_mask = imread(mask_path + mask)\n",
    "    label = input_mask\n",
    "    #Add pixel values to the data frame\n",
    "    label_values = label.reshape(-1)\n",
    "    df2['Label_Value'] = label_values\n",
    "    df2['Mask_Name'] = mask\n",
    "    mask_dataset = mask_dataset.append(df2)  #Update mask dataframe with all the info from each mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d302828",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    " #  STEP 3: GET DATA READY FOR RANDOM FOREST (or other classifier)\n",
    " # COMBINE BOTH DATAFRAMES INTO A SINGLE DATASET\n",
    "###############################################################\n",
    "dataset = pd.concat([image_dataset, mask_dataset], axis=1)    #Concatenate both image and mask datasets\n",
    "\n",
    "#If you expect image and mask names to be the same this is where we can perform sanity check\n",
    "#dataset['Image_Name'].equals(dataset['Mask_Name'])   \n",
    "##\n",
    "##If we do not want to include pixels with value 0 \n",
    "##e.g. Sometimes unlabeled pixels may be given a value 0.\n",
    "# dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# dataset.fillna(999, inplace=True)\n",
    "dataset.fillna(-1, inplace=True)\n",
    "dataset = dataset[dataset.Label_Value != -1]\n",
    "dataset.dropna()\n",
    "# dataset = dataset.reset_index()\n",
    "# dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# dataset.fillna(999, inplace=True)\n",
    "\n",
    "\n",
    "#Assign training features to X and labels to Y\n",
    "#Drop columns that are not relevant for training (non-features)\n",
    "X = dataset.drop(labels = [\"Mask_Name\", \"Label_Value\"], axis=1) \n",
    "\n",
    "#Assign label values to Y (our prediction)\n",
    "Y = dataset[\"Label_Value\"].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity Check\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    " #  STEP : Scale The Features\n",
    "###############################################################\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0523785",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split data into train and test to verify accuracy after fitting the model. \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.3) #random_state=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5937692",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# STEP 4: Define the classifier and fit a model with our training data\n",
    "###################################################################\n",
    "\n",
    "#Import training classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Instantiate model with n number of decision trees\n",
    "model = RandomForestClassifier(n_estimators = 20, verbose=10)\n",
    "\n",
    "## Train the model on training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# STEP 5: Accuracy check - BINARY CLASSIFICATION\n",
    "#########################################################\n",
    "\n",
    "from sklearn import metrics\n",
    "prediction_test = model.predict(X_test)\n",
    "##Check accuracy on test dataset. \n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))\n",
    "# print(\"IOU = \", metrics.jaccard_score(y_test, prediction_test))\n",
    "print(\"Precision = \", metrics.precision_score(y_test, prediction_test))\n",
    "print(\"Recall = \", \tmetrics.recall_score(y_test, prediction_test))\n",
    "print(\"F1 score = \", metrics.f1_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# STEP 5: Accuracy check - MULTILABEL CLASSIFICATION\n",
    "#########################################################\n",
    "\n",
    "from sklearn import metrics\n",
    "prediction_test = model.predict(X_test)\n",
    "##Check accuracy on test dataset. \n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))\n",
    "print(\"IOU = \", metrics.jaccard_score(y_test, prediction_test, average =\"micro\"))\n",
    "print(\"Precision = \", metrics.precision_score(y_test, prediction_test, average =\"micro\"))\n",
    "print(\"Recall = \", \tmetrics.recall_score(y_test, prediction_test, average =\"micro\"))\n",
    "print(\"F1 score = \", metrics.f1_score(y_test, prediction_test, average =\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857297d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# STEP 6: Plot Feature Importance\n",
    "#########################################################\n",
    "\n",
    "# plt.barh(image_dataset.columns, model.feature_importances_)\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "sorted_idx = model.feature_importances_.argsort()\n",
    "plt.barh(image_dataset.columns[sorted_idx], model.feature_importances_[sorted_idx])\n",
    "# plt.margins(x=0, y=-0.10) \n",
    "plt.xlabel(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c57f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982760e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bafcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
